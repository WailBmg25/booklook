{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\udcda Institutional Books Dataset Loader - FIXED VERSION\n",
    "\n",
    "Load books from HuggingFace **institutional/institutional-books-1.0** dataset into BookLook database.\n",
    "\n",
    "## \u2705 FIXES APPLIED:\n",
    "1. **Better cover image fetching** - Uses GET request with content-length check + Google Books API fallback\n",
    "2. **Auto-generate descriptions** - Creates descriptions from book content if missing\n",
    "3. **Author linking** - Creates Author records and links them to books properly\n",
    "4. **Book content loading** - Loads actual book pages from dataset's `text` field\n",
    "\n",
    "## Workflow\n",
    "1. Load data from dataset (streaming)\n",
    "2. Structure with pandas DataFrame\n",
    "3. Create Author records\n",
    "4. Create Book records with proper author linking\n",
    "5. Create BookPage records from actual book content\n",
    "\n",
    "## Field Mapping\n",
    "Dataset \u2192 Database:\n",
    "- `title_src` \u2192 `titre`\n",
    "- `author_src` \u2192 Author record + book_authors link\n",
    "- `date1_src` \u2192 `date_publication`\n",
    "- `page_count_src` \u2192 `nombre_pages`, `total_pages`\n",
    "- `language_gen` \u2192 `langue`\n",
    "- `general_note_src` OR generated \u2192 `description`\n",
    "- `text` \u2192 BookPage records (actual content!)\n",
    "- `token_count_o200k_base_gen` \u2192 `word_count` (\u00d70.75)\n",
    "- `genre_or_form_src` + `topic_or_subject_gen` \u2192 `genre_names` (array)\n",
    "- `identifiers_src.isbn` \u2192 `isbn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets huggingface-hub psycopg2-binary requests pandas -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from typing import List, Dict, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "import requests\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "\n",
    "print(\"\u2705 All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configuration (\u26a0\ufe0f UPDATE YOUR TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u26a0\ufe0f UPDATE THIS: Your HuggingFace token\n",
    "HF_TOKEN = \"YOUR_HUGGINGFACE_TOKEN_HERE\"\n",
    "\n",
    "# Dataset configuration\n",
    "DATASET_NAME = \"institutional/institutional-books-1.0\"\n",
    "CHUNK_SIZE = 50  # Books per batch (reduced for better memory management)\n",
    "MAX_CHUNKS = 10   # Set to None for all books\n",
    "WORDS_PER_PAGE = 500  # Words per page when splitting content\n",
    "\n",
    "# Database configuration\n",
    "DB_CONFIG = {\n",
    "    'host': 'localhost',\n",
    "    'port': 5432,\n",
    "    'database': 'book_library',\n",
    "    'user': 'bookuser',\n",
    "    'password': 'bookpass123'\n",
    "}\n",
    "\n",
    "# Progress tracking\n",
    "PROGRESS_FILE = 'load_progress.json'\n",
    "\n",
    "print(f\"\ud83d\udcda Dataset: {DATASET_NAME}\")\n",
    "print(f\"\ud83d\udce6 Chunk size: {CHUNK_SIZE}\")\n",
    "print(f\"\ud83d\udd22 Max chunks: {MAX_CHUNKS if MAX_CHUNKS else 'All'}\")\n",
    "print(f\"\ud83d\udcc4 Words per page: {WORDS_PER_PAGE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_cover_image(dataset_image_url: Optional[str], isbn: Optional[str], timeout: int = 5) -> Optional[str]:\n",
    "    \"\"\"Fetch book cover image with fallback to Google Books API.\"\"\"\n",
    "    \n",
    "    # Try dataset image URL first\n",
    "    if dataset_image_url:\n",
    "        try:\n",
    "            response = requests.head(dataset_image_url, timeout=timeout, allow_redirects=True)\n",
    "            if response.status_code == 200:\n",
    "                content_type = response.headers.get('content-type', '')\n",
    "                if 'image' in content_type.lower():\n",
    "                    print(f\"  \u2713 Using dataset image\")\n",
    "                    return dataset_image_url\n",
    "        except Exception as e:\n",
    "            print(f\"  \u26a0 Dataset image failed: {e}\")\n",
    "    \n",
    "    # Fallback to Google Books API\n",
    "    if isbn:\n",
    "        try:\n",
    "            # Clean ISBN (remove hyphens)\n",
    "            clean_isbn = isbn.replace('-', '').replace(' ', '')\n",
    "            google_api_url = f\"https://www.googleapis.com/books/v1/volumes?q=isbn:{clean_isbn}\"\n",
    "            \n",
    "            response = requests.get(google_api_url, timeout=timeout)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if data.get('totalItems', 0) > 0:\n",
    "                    volume_info = data['items'][0].get('volumeInfo', {})\n",
    "                    image_links = volume_info.get('imageLinks', {})\n",
    "                    \n",
    "                    # Try different image sizes\n",
    "                    for size in ['large', 'medium', 'small', 'thumbnail', 'smallThumbnail']:\n",
    "                        if size in image_links:\n",
    "                            print(f\"  \u2713 Using Google Books image ({size})\")\n",
    "                            return image_links[size]\n",
    "        except Exception as e:\n",
    "            print(f\"  \u26a0 Google Books API failed: {e}\")\n",
    "    \n",
    "    print(f\"  \u2717 No cover image found\")\n",
    "    return None\n",
    "\n",
    "print(\"\u2705 Cover fetching function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_description(text: Optional[str], max_words: int = 250) -> str:\n",
    "    \"\"\"Generate description from book text content.\"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return \"No description available.\"\n",
    "    \n",
    "    # Clean the text\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Split into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Take first max_words\n",
    "    if len(words) <= max_words:\n",
    "        return text\n",
    "    \n",
    "    # Extract and join\n",
    "    description = ' '.join(words[:max_words])\n",
    "    \n",
    "    # Try to end at a sentence boundary\n",
    "    last_period = description.rfind('.')\n",
    "    last_exclamation = description.rfind('!')\n",
    "    last_question = description.rfind('?')\n",
    "    \n",
    "    last_sentence_end = max(last_period, last_exclamation, last_question)\n",
    "    \n",
    "    if last_sentence_end > len(description) * 0.7:  # If we're at least 70% through\n",
    "        description = description[:last_sentence_end + 1]\n",
    "    else:\n",
    "        description += '...'\n",
    "    \n",
    "    return description\n",
    "\n",
    "print(\"\u2705 Description generation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_pages(text: str, words_per_page: int = 500) -> List[str]:\n",
    "    \"\"\"Split book text into pages based on word count.\"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return []\n",
    "    \n",
    "    words = text.split()\n",
    "    pages = []\n",
    "    \n",
    "    for i in range(0, len(words), words_per_page):\n",
    "        page_words = words[i:i + words_per_page]\n",
    "        page_content = ' '.join(page_words)\n",
    "        pages.append(page_content)\n",
    "    \n",
    "    return pages\n",
    "\n",
    "print(\"\u2705 Page splitting function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_author_name(author_str: str) -> tuple:\n",
    "    \"\"\"Parse author string into first name and last name.\"\"\"\n",
    "    if not author_str or not author_str.strip():\n",
    "        return (\"Unknown\", \"Author\")\n",
    "    \n",
    "    author_str = author_str.strip()\n",
    "    \n",
    "    # Split by comma (Last, First format)\n",
    "    if ',' in author_str:\n",
    "        parts = author_str.split(',', 1)\n",
    "        last_name = parts[0].strip()\n",
    "        first_name = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "        return (first_name, last_name)\n",
    "    \n",
    "    # Split by space (First Last format)\n",
    "    parts = author_str.split()\n",
    "    if len(parts) == 1:\n",
    "        return (\"\", parts[0])\n",
    "    elif len(parts) == 2:\n",
    "        return (parts[0], parts[1])\n",
    "    else:\n",
    "        # Multiple parts - assume first is first name, rest is last name\n",
    "        return (parts[0], ' '.join(parts[1:]))\n",
    "\n",
    "print(\"\u2705 Author parsing function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database\n",
    "try:\n",
    "    conn = psycopg2.connect(**DB_CONFIG)\n",
    "    cursor = conn.cursor()\n",
    "    print(\"\u2705 Connected to database\")\n",
    "    \n",
    "    # Test query\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM books\")\n",
    "    book_count = cursor.fetchone()[0]\n",
    "    print(f\"\ud83d\udcda Current books in database: {book_count}\")\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Database connection failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Load Progress Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_progress():\n",
    "    \"\"\"Load progress from file.\"\"\"\n",
    "    if os.path.exists(PROGRESS_FILE):\n",
    "        with open(PROGRESS_FILE, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {'processed_books': 0, 'last_index': 0, 'errors': []}\n",
    "\n",
    "def save_progress(progress):\n",
    "    \"\"\"Save progress to file.\"\"\"\n",
    "    with open(PROGRESS_FILE, 'w') as f:\n",
    "        json.dump(progress, f, indent=2)\n",
    "\n",
    "progress = load_progress()\n",
    "print(f\"\ud83d\udcca Progress loaded: {progress['processed_books']} books processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Login to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace\n",
    "try:\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"\u2705 Logged in to HuggingFace\")\n",
    "except Exception as e:\n",
    "    print(f\"\u274c HuggingFace login failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Load Dataset and Process Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset in streaming mode\n",
    "print(f\"\ud83d\udce5 Loading dataset: {DATASET_NAME}\")\n",
    "dataset = load_dataset(DATASET_NAME, split='train', streaming=True)\n",
    "\n",
    "# Process books in chunks\n",
    "books_processed = 0\n",
    "chunks_processed = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for i, book_data in enumerate(dataset):\n",
    "    # Skip already processed books\n",
    "    if i < progress['last_index']:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"\ud83d\udcd6 Processing book {i+1}: {book_data.get('title_src', 'Unknown')}\")\n",
    "        \n",
    "        # Extract book data\n",
    "        title = book_data.get('title_src', 'Unknown Title')\n",
    "        author_src = book_data.get('author_src', 'Unknown Author')\n",
    "        date_pub = book_data.get('date1_src')\n",
    "        page_count = book_data.get('page_count_src')\n",
    "        language = book_data.get('language_gen', 'en')\n",
    "        general_note = book_data.get('general_note_src', '')\n",
    "        text_content = book_data.get('text', '')\n",
    "        token_count = book_data.get('token_count_o200k_base_gen', 0)\n",
    "        genre_form = book_data.get('genre_or_form_src', [])\n",
    "        topic_subject = book_data.get('topic_or_subject_gen', [])\n",
    "        identifiers = book_data.get('identifiers_src', {})\n",
    "        \n",
    "        # Extract ISBN\n",
    "        isbn = identifiers.get('isbn', [None])[0] if isinstance(identifiers.get('isbn'), list) else identifiers.get('isbn')\n",
    "        if not isbn:\n",
    "            isbn = f\"INST-{i:08d}\"  # Generate unique ISBN\n",
    "        \n",
    "        # Check if book already exists\n",
    "        cursor.execute(\"SELECT id FROM books WHERE isbn = %s\", (isbn,))\n",
    "        if cursor.fetchone():\n",
    "            print(f\"  \u23ed\ufe0f  Book already exists (ISBN: {isbn})\")\n",
    "            continue\n",
    "        \n",
    "        # Generate description if missing\n",
    "        description = general_note if general_note and general_note.strip() else generate_description(text_content)\n",
    "        print(f\"  \ud83d\udcdd Description: {description[:100]}...\")\n",
    "        \n",
    "        # Fetch cover image\n",
    "        image_url = fetch_cover_image(None, isbn)\n",
    "        \n",
    "        # Calculate word count\n",
    "        word_count = int(token_count * 0.75) if token_count else len(text_content.split())\n",
    "        \n",
    "        # Combine genres\n",
    "        genres = []\n",
    "        if isinstance(genre_form, list):\n",
    "            genres.extend(genre_form)\n",
    "        if isinstance(topic_subject, list):\n",
    "            genres.extend(topic_subject[:2])  # Limit topics\n",
    "        genres = list(set(genres))[:5]  # Unique, max 5\n",
    "        \n",
    "        # Parse author\n",
    "        first_name, last_name = parse_author_name(author_src)\n",
    "        author_full_name = f\"{first_name} {last_name}\".strip()\n",
    "        \n",
    "        # Create or get author\n",
    "        cursor.execute(\n",
    "            \"SELECT id FROM authors WHERE nom = %s AND prenom = %s\",\n",
    "            (last_name, first_name)\n",
    "        )\n",
    "        author_result = cursor.fetchone()\n",
    "        \n",
    "        if author_result:\n",
    "            author_id = author_result[0]\n",
    "            print(f\"  \ud83d\udc64 Found existing author: {author_full_name}\")\n",
    "        else:\n",
    "            cursor.execute(\n",
    "                \"INSERT INTO authors (nom, prenom) VALUES (%s, %s) RETURNING id\",\n",
    "                (last_name, first_name)\n",
    "            )\n",
    "            author_id = cursor.fetchone()[0]\n",
    "            print(f\"  \ud83d\udc64 Created author: {author_full_name}\")\n",
    "        \n",
    "        # Insert book\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO books (\n",
    "                titre, isbn, date_publication, description, image_url,\n",
    "                nombre_pages, langue, author_names, genre_names,\n",
    "                word_count, total_pages, average_rating, review_count\n",
    "            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "            RETURNING id\n",
    "        \"\"\", (\n",
    "            title, isbn, date_pub, description, image_url,\n",
    "            page_count, language, [author_full_name], genres,\n",
    "            word_count, 0, 0, 0\n",
    "        ))\n",
    "        book_id = cursor.fetchone()[0]\n",
    "        print(f\"  \ud83d\udcda Created book (ID: {book_id})\")\n",
    "        \n",
    "        # Link book to author\n",
    "        cursor.execute(\n",
    "            \"INSERT INTO book_author_association (book_id, author_id) VALUES (%s, %s)\",\n",
    "            (book_id, author_id)\n",
    "        )\n",
    "        \n",
    "        # Split and insert book pages\n",
    "        if text_content and text_content.strip():\n",
    "            pages = split_text_into_pages(text_content, WORDS_PER_PAGE)\n",
    "            print(f\"  \ud83d\udcc4 Creating {len(pages)} pages...\")\n",
    "            \n",
    "            page_data = []\n",
    "            for page_num, page_content in enumerate(pages, start=1):\n",
    "                page_word_count = len(page_content.split())\n",
    "                page_data.append((book_id, page_num, page_content, page_word_count))\n",
    "            \n",
    "            # Bulk insert pages\n",
    "            execute_values(\n",
    "                cursor,\n",
    "                \"INSERT INTO book_pages (book_id, page_number, content, word_count) VALUES %s\",\n",
    "                page_data\n",
    "            )\n",
    "            \n",
    "            # Update book's total_pages\n",
    "            cursor.execute(\n",
    "                \"UPDATE books SET total_pages = %s WHERE id = %s\",\n",
    "                (len(pages), book_id)\n",
    "            )\n",
    "            print(f\"  \u2705 Added {len(pages)} pages\")\n",
    "        \n",
    "        # Commit transaction\n",
    "        conn.commit()\n",
    "        \n",
    "        books_processed += 1\n",
    "        progress['processed_books'] = books_processed\n",
    "        progress['last_index'] = i + 1\n",
    "        \n",
    "        print(f\"  \u2705 Book processed successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  \u274c Error processing book: {e}\")\n",
    "        progress['errors'].append({'index': i, 'title': book_data.get('title_src'), 'error': str(e)})\n",
    "        conn.rollback()\n",
    "        continue\n",
    "    \n",
    "    # Save progress every 10 books\n",
    "    if books_processed % 10 == 0:\n",
    "        save_progress(progress)\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = books_processed / elapsed if elapsed > 0 else 0\n",
    "        print(f\"\\n\ud83d\udcca Progress: {books_processed} books | {rate:.2f} books/sec\")\n",
    "    \n",
    "    # Check if we've reached the chunk limit\n",
    "    if books_processed >= CHUNK_SIZE:\n",
    "        chunks_processed += 1\n",
    "        if MAX_CHUNKS and chunks_processed >= MAX_CHUNKS:\n",
    "            print(f\"\\n\ud83d\uded1 Reached maximum chunks ({MAX_CHUNKS})\")\n",
    "            break\n",
    "        books_processed = 0\n",
    "\n",
    "# Final save\n",
    "save_progress(progress)\n",
    "\n",
    "# Close database connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"\u2705 Processing complete!\")\n",
    "print(f\"\ud83d\udcda Total books processed: {progress['processed_books']}\")\n",
    "print(f\"\u274c Errors: {len(progress['errors'])}\")\n",
    "print(f\"\u23f1\ufe0f  Total time: {time.time() - start_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}