{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Institutional Books Dataset Loader\n",
    "\n",
    "Load books from HuggingFace **institutional/institutional-books-1.0** dataset into BookLook database.\n",
    "\n",
    "## Features\n",
    "- ‚úÖ Streaming mode (memory efficient)\n",
    "- ‚úÖ Chunked loading (100 books per batch)\n",
    "- ‚úÖ Resume capability\n",
    "- ‚úÖ ISBN lookup (dataset ‚Üí Google Books ‚Üí Open Library ‚Üí generated)\n",
    "- ‚úÖ Cover image fetching\n",
    "- ‚úÖ Transaction rollback on errors\n",
    "\n",
    "## Instructions\n",
    "1. Run cells in order\n",
    "2. Update HuggingFace token in Configuration cell\n",
    "3. Adjust MAX_CHUNKS for testing (10 = 1000 books)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets huggingface-hub psycopg2-binary requests python-dotenv -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from typing import List, Dict, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import requests\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configuration (‚ö†Ô∏è UPDATE YOUR TOKEN HERE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è UPDATE THIS: Your HuggingFace token\n",
    "HF_TOKEN = \"YOUR_HUGGINGFACE_TOKEN_HERE\"\n",
    "\n",
    "# Dataset configuration\n",
    "DATASET_NAME = \"institutional/institutional-books-1.0\"\n",
    "CHUNK_SIZE = 100  # Books per batch\n",
    "MAX_CHUNKS = 10   # Set to None for all books (10 = 1000 books for testing)\n",
    "\n",
    "# Database configuration\n",
    "DB_CONFIG = {\n",
    "    'host': 'localhost',\n",
    "    'port': 5432,\n",
    "    'database': 'book_library',\n",
    "    'user': 'bookuser',\n",
    "    'password': 'bookpass123'\n",
    "}\n",
    "\n",
    "# API configuration\n",
    "API_DELAY = 0.5  # Seconds between API calls\n",
    "GOOGLE_BOOKS_API_KEY = ''  # Optional\n",
    "PROGRESS_FILE = 'load_progress.json'\n",
    "\n",
    "print(f\"üìö Dataset: {DATASET_NAME}\")\n",
    "print(f\"üì¶ Chunk size: {CHUNK_SIZE}\")\n",
    "print(f\"üî¢ Max chunks: {MAX_CHUNKS if MAX_CHUNKS else 'All'}\")\n",
    "print(f\"üîå Database: {DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_isbn_from_identifiers(identifiers: Dict) -> Optional[str]:\n",
    "    if not identifiers or 'isbn' not in identifiers:\n",
    "        return None\n",
    "    isbns = identifiers['isbn']\n",
    "    if isinstance(isbns, list) and len(isbns) > 0:\n",
    "        isbn = str(isbns[0]).strip().replace('-', '').replace(' ', '')\n",
    "        if len(isbn) in [10, 13]:\n",
    "            return isbn\n",
    "    return None\n",
    "\n",
    "def search_isbn_google_books(title: str, author: str) -> Optional[str]:\n",
    "    if not title:\n",
    "        return None\n",
    "    try:\n",
    "        query = f\"{title} {author}\" if author else title\n",
    "        url = \"https://www.googleapis.com/books/v1/volumes\"\n",
    "        params = {'q': query, 'maxResults': 1}\n",
    "        if GOOGLE_BOOKS_API_KEY:\n",
    "            params['key'] = GOOGLE_BOOKS_API_KEY\n",
    "        response = requests.get(url, params=params, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if 'items' in data and len(data['items']) > 0:\n",
    "                identifiers = data['items'][0].get('volumeInfo', {}).get('industryIdentifiers', [])\n",
    "                for identifier in identifiers:\n",
    "                    if identifier.get('type') == 'ISBN_13':\n",
    "                        return identifier.get('identifier')\n",
    "                for identifier in identifiers:\n",
    "                    if identifier.get('type') == 'ISBN_10':\n",
    "                        return identifier.get('identifier')\n",
    "        time.sleep(API_DELAY)\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def search_isbn_open_library(title: str, author: str) -> Optional[str]:\n",
    "    if not title:\n",
    "        return None\n",
    "    try:\n",
    "        query = f\"{title} {author}\" if author else title\n",
    "        url = \"https://openlibrary.org/search.json\"\n",
    "        response = requests.get(url, params={'q': query, 'limit': 1}, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if 'docs' in data and len(data['docs']) > 0:\n",
    "                doc = data['docs'][0]\n",
    "                if 'isbn' in doc and doc['isbn']:\n",
    "                    for isbn in doc['isbn']:\n",
    "                        isbn_clean = str(isbn).strip().replace('-', '')\n",
    "                        if len(isbn_clean) == 13:\n",
    "                            return isbn_clean\n",
    "                    return str(doc['isbn'][0]).strip().replace('-', '')\n",
    "        time.sleep(API_DELAY)\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def generate_isbn_from_barcode(barcode: str) -> str:\n",
    "    barcode_hash = abs(hash(barcode)) % (10 ** 10)\n",
    "    return f\"999{barcode_hash:010d}\"\n",
    "\n",
    "def get_isbn_for_book(book_data: Dict) -> str:\n",
    "    isbn = extract_isbn_from_identifiers(book_data.get('identifiers_src'))\n",
    "    if isbn:\n",
    "        return isbn\n",
    "    title = book_data.get('title_src', '')\n",
    "    author = book_data.get('author_src', '')\n",
    "    isbn = search_isbn_google_books(title, author)\n",
    "    if isbn:\n",
    "        return isbn\n",
    "    isbn = search_isbn_open_library(title, author)\n",
    "    if isbn:\n",
    "        return isbn\n",
    "    return generate_isbn_from_barcode(book_data.get('barcode_src', ''))\n",
    "\n",
    "def fetch_cover_image(isbn: str) -> Optional[str]:\n",
    "    try:\n",
    "        url = f\"https://covers.openlibrary.org/b/isbn/{isbn}-L.jpg\"\n",
    "        response = requests.head(url, timeout=3)\n",
    "        if response.status_code == 200:\n",
    "            return url\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def parse_publication_date(date_str: str) -> Optional[str]:\n",
    "    if not date_str:\n",
    "        return None\n",
    "    year_match = re.search(r'\\b(1[0-9]{3}|20[0-9]{2})\\b', str(date_str))\n",
    "    if year_match:\n",
    "        return f\"{year_match.group(1)}-01-01\"\n",
    "    return None\n",
    "\n",
    "def clean_text(text: str, max_length: int = 5000) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    if len(text) > max_length:\n",
    "        text = text[:max_length] + \"...\"\n",
    "    return text\n",
    "\n",
    "def extract_language(book_data: Dict) -> str:\n",
    "    lang = book_data.get('language_gen', '') or book_data.get('language_src', '')\n",
    "    lang_map = {\n",
    "        'eng': 'English', 'fra': 'French', 'deu': 'German',\n",
    "        'spa': 'Spanish', 'ita': 'Italian', 'por': 'Portuguese'\n",
    "    }\n",
    "    return lang_map.get(lang[:3].lower(), lang or 'English')\n",
    "\n",
    "def extract_genres(book_data: Dict) -> List[str]:\n",
    "    genres = []\n",
    "    genre_str = book_data.get('genre_or_form_src', '')\n",
    "    if genre_str:\n",
    "        parts = re.split(r'[;,|]', genre_str)\n",
    "        genres.extend([g.strip() for g in parts if g.strip()])\n",
    "    topic = book_data.get('topic_or_subject_gen') or book_data.get('topic_or_subject_src', '')\n",
    "    if topic and topic not in genres:\n",
    "        genres.append(topic)\n",
    "    return genres[:3] if genres else ['General']\n",
    "\n",
    "def extract_description(book_data: Dict) -> str:\n",
    "    desc = book_data.get('general_note_src', '')\n",
    "    if not desc:\n",
    "        parts = []\n",
    "        topic = book_data.get('topic_or_subject_gen') or book_data.get('topic_or_subject_src')\n",
    "        if topic:\n",
    "            parts.append(f\"Subject: {topic}\")\n",
    "        pages = book_data.get('page_count_src')\n",
    "        if pages:\n",
    "            parts.append(f\"{pages} pages\")\n",
    "        desc = \". \".join(parts) if parts else \"No description available\"\n",
    "    return clean_text(desc, 2000)\n",
    "\n",
    "print(\"‚úÖ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Database Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_author(cursor, author_name: str) -> int:\n",
    "    if not author_name or author_name.strip() == '':\n",
    "        author_name = \"Unknown Author\"\n",
    "    parts = author_name.strip().split()\n",
    "    if len(parts) >= 2:\n",
    "        prenom = ' '.join(parts[:-1])\n",
    "        nom = parts[-1]\n",
    "    else:\n",
    "        prenom = \"\"\n",
    "        nom = author_name.strip()\n",
    "    cursor.execute(\"SELECT id FROM authors WHERE nom = %s AND prenom = %s\", (nom, prenom))\n",
    "    result = cursor.fetchone()\n",
    "    if result:\n",
    "        return result[0]\n",
    "    cursor.execute(\n",
    "        \"INSERT INTO authors (nom, prenom, created_at) VALUES (%s, %s, NOW()) RETURNING id\",\n",
    "        (nom, prenom)\n",
    "    )\n",
    "    return cursor.fetchone()[0]\n",
    "\n",
    "def get_or_create_genre(cursor, genre_name: str) -> int:\n",
    "    if not genre_name or genre_name.strip() == '':\n",
    "        genre_name = \"General\"\n",
    "    genre_name = genre_name.strip()\n",
    "    cursor.execute(\"SELECT id FROM genres WHERE nom = %s\", (genre_name,))\n",
    "    result = cursor.fetchone()\n",
    "    if result:\n",
    "        return result[0]\n",
    "    cursor.execute(\n",
    "        \"INSERT INTO genres (nom, created_at) VALUES (%s, NOW()) RETURNING id\",\n",
    "        (genre_name,)\n",
    "    )\n",
    "    return cursor.fetchone()[0]\n",
    "\n",
    "print(\"‚úÖ Database functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Progress Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_progress() -> Dict:\n",
    "    if os.path.exists(PROGRESS_FILE):\n",
    "        with open(PROGRESS_FILE, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {'last_processed_index': -1, 'total_loaded': 0, 'last_barcode': None, 'timestamp': None}\n",
    "\n",
    "def save_progress(index: int, total: int, barcode: str):\n",
    "    progress = {\n",
    "        'last_processed_index': index,\n",
    "        'total_loaded': total,\n",
    "        'last_barcode': barcode,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    with open(PROGRESS_FILE, 'w') as f:\n",
    "        json.dump(progress, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Progress tracking functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Login to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login(token=HF_TOKEN)\n",
    "print(\"‚úÖ Successfully logged in to HuggingFace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Load Dataset with Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress = load_progress()\n",
    "start_index = progress['last_processed_index'] + 1\n",
    "\n",
    "if start_index > 0:\n",
    "    print(f\"üìç Resuming from index {start_index} ({progress['total_loaded']} books loaded)\")\n",
    "    print(f\"   Last: {progress.get('last_barcode', 'N/A')} at {progress.get('timestamp', 'N/A')}\")\n",
    "\n",
    "print(f\"\\nüì• Loading dataset: {DATASET_NAME}\")\n",
    "dataset_stream = load_dataset(DATASET_NAME, split=\"train\", streaming=True)\n",
    "print(\"‚úÖ Dataset loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Connect to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(**DB_CONFIG)\n",
    "conn.set_session(autocommit=False)\n",
    "cursor = conn.cursor()\n",
    "print(\"‚úÖ Database connected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Load Books (Main Processing Loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüöÄ Starting data loading (chunk size: {CHUNK_SIZE})\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "chunk_buffer = []\n",
    "current_index = 0\n",
    "chunk_number = 0\n",
    "total_inserted = progress['total_loaded']\n",
    "total_skipped = 0\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    for book_data in dataset_stream:\n",
    "        if current_index < start_index:\n",
    "            current_index += 1\n",
    "            continue\n",
    "        \n",
    "        chunk_buffer.append(book_data)\n",
    "        current_index += 1\n",
    "        \n",
    "        if len(chunk_buffer) >= CHUNK_SIZE:\n",
    "            chunk_number += 1\n",
    "            print(f\"\\nüì¶ Chunk {chunk_number} (books {current_index - CHUNK_SIZE + 1}-{current_index})\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            inserted = 0\n",
    "            skipped = 0\n",
    "            \n",
    "            for book in chunk_buffer:\n",
    "                try:\n",
    "                    isbn = get_isbn_for_book(book)\n",
    "                    cursor.execute(\"SELECT id FROM books WHERE isbn = %s\", (isbn,))\n",
    "                    if cursor.fetchone():\n",
    "                        skipped += 1\n",
    "                        continue\n",
    "                    \n",
    "                    title = book.get('title_src', 'Unknown Title')\n",
    "                    author_name = book.get('author_src', 'Unknown Author')\n",
    "                    pub_date = parse_publication_date(book.get('date1_src', ''))\n",
    "                    description = extract_description(book)\n",
    "                    page_count = book.get('page_count_src')\n",
    "                    language = extract_language(book)\n",
    "                    genres = extract_genres(book)\n",
    "                    cover_url = fetch_cover_image(isbn)\n",
    "                    token_count = book.get('token_count_o200k_base_gen', 0)\n",
    "                    word_count = int(token_count * 0.75) if token_count else None\n",
    "                    \n",
    "                    cursor.execute(\n",
    "                        \"\"\"\n",
    "                        INSERT INTO books (\n",
    "                            titre, isbn, date_publication, description, image_url,\n",
    "                            nombre_pages, total_pages, langue, note_moyenne, nombre_reviews,\n",
    "                            average_rating, review_count, word_count, created_at\n",
    "                        )\n",
    "                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW())\n",
    "                        RETURNING id\n",
    "                        \"\"\",\n",
    "                        (title, isbn, pub_date, description, cover_url,\n",
    "                         page_count, page_count, language, 0.0, 0, 0.0, 0, word_count)\n",
    "                    )\n",
    "                    book_id = cursor.fetchone()[0]\n",
    "                    \n",
    "                    author_id = get_or_create_author(cursor, author_name)\n",
    "                    cursor.execute(\n",
    "                        \"INSERT INTO book_authors (book_id, author_id) VALUES (%s, %s) ON CONFLICT DO NOTHING\",\n",
    "                        (book_id, author_id)\n",
    "                    )\n",
    "                    \n",
    "                    for genre_name in genres:\n",
    "                        genre_id = get_or_create_genre(cursor, genre_name)\n",
    "                        cursor.execute(\n",
    "                            \"INSERT INTO book_genres (book_id, genre_id) VALUES (%s, %s) ON CONFLICT DO NOTHING\",\n",
    "                            (book_id, genre_id)\n",
    "                        )\n",
    "                    \n",
    "                    cursor.execute(\n",
    "                        \"UPDATE books SET author_names = ARRAY[%s], genre_names = %s WHERE id = %s\",\n",
    "                        (author_name, genres, book_id)\n",
    "                    )\n",
    "                    \n",
    "                    inserted += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"    ‚ö†Ô∏è  Error: {str(e)[:100]}\")\n",
    "                    conn.rollback()\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "            \n",
    "            conn.commit()\n",
    "            \n",
    "            total_inserted += inserted\n",
    "            total_skipped += skipped\n",
    "            \n",
    "            last_barcode = chunk_buffer[-1].get('barcode_src', 'unknown')\n",
    "            save_progress(current_index - 1, total_inserted, last_barcode)\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            rate = total_inserted / elapsed if elapsed > 0 else 0\n",
    "            \n",
    "            print(f\"   ‚úÖ Inserted: {inserted}\")\n",
    "            print(f\"   ‚è≠Ô∏è  Skipped: {skipped}\")\n",
    "            print(f\"   üìä Total: {total_inserted} loaded, {total_skipped} skipped\")\n",
    "            print(f\"   ‚è±Ô∏è  Rate: {rate:.1f} books/sec\")\n",
    "            print(f\"   üíæ Progress saved\")\n",
    "            \n",
    "            chunk_buffer = []\n",
    "            \n",
    "            if MAX_CHUNKS and chunk_number >= MAX_CHUNKS:\n",
    "                print(f\"\\nüèÅ Reached max chunks ({MAX_CHUNKS})\")\n",
    "                break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚ö†Ô∏è  Interrupted by user\")\n",
    "    conn.commit()\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error: {e}\")\n",
    "    conn.rollback()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ LOADING COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"üìä Total inserted: {total_inserted}\")\n",
    "print(f\"‚è≠Ô∏è  Total skipped: {total_skipped}\")\n",
    "print(f\"üì¶ Chunks processed: {chunk_number}\")\n",
    "print(f\"‚è±Ô∏è  Time: {elapsed:.1f}s\")\n",
    "print(f\"üìà Rate: {total_inserted / elapsed:.1f} books/sec\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Close Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.close()\n",
    "conn.close()\n",
    "print(\"üîå Database connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: View Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(**DB_CONFIG)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "print(\"\\nüìä Database Summary:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "cursor.execute(\"SELECT COUNT(*) FROM books\")\n",
    "print(f\"üìö Total Books: {cursor.fetchone()[0]}\")\n",
    "\n",
    "cursor.execute(\"SELECT COUNT(*) FROM authors\")\n",
    "print(f\"‚úçÔ∏è  Total Authors: {cursor.fetchone()[0]}\")\n",
    "\n",
    "cursor.execute(\"SELECT COUNT(*) FROM genres\")\n",
    "print(f\"üè∑Ô∏è  Total Genres: {cursor.fetchone()[0]}\")\n",
    "\n",
    "cursor.execute(\"SELECT COUNT(*) FROM books WHERE image_url IS NOT NULL\")\n",
    "print(f\"üñºÔ∏è  Books with covers: {cursor.fetchone()[0]}\")\n",
    "\n",
    "cursor.execute(\"SELECT langue, COUNT(*) FROM books GROUP BY langue ORDER BY COUNT(*) DESC LIMIT 5\")\n",
    "print(\"\\nüåç Top 5 Languages:\")\n",
    "for lang, count in cursor.fetchall():\n",
    "    print(f\"   {lang}: {count} books\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n",
    "print(\"\\n‚úÖ Summary complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
