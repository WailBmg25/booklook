{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\udcda Institutional Books Dataset Loader\n",
    "\n",
    "Load books from HuggingFace **institutional/institutional-books-1.0** dataset into BookLook database.\n",
    "\n",
    "## Workflow\n",
    "1. Load data from dataset (streaming)\n",
    "2. Structure with pandas DataFrame\n",
    "3. Rename fields to match database schema\n",
    "4. Push to database in batches\n",
    "\n",
    "## Field Mapping\n",
    "Dataset \u2192 Database:\n",
    "- `title_src` \u2192 `titre`\n",
    "- `author_src` \u2192 `author_names` (array)\n",
    "- `date1_src` \u2192 `date_publication`\n",
    "- `page_count_src` \u2192 `nombre_pages`, `total_pages`\n",
    "- `language_gen` \u2192 `langue`\n",
    "- `general_note_src` \u2192 `description`\n",
    "- `token_count_o200k_base_gen` \u2192 `word_count` (\u00d70.75)\n",
    "- `genre_or_form_src` + `topic_or_subject_gen` \u2192 `genre_names` (array)\n",
    "- `identifiers_src.isbn` \u2192 `isbn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets huggingface-hub psycopg2-binary requests pandas -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from typing import List, Dict, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "import requests\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "\n",
    "print(\"\u2705 All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configuration (\u26a0\ufe0f UPDATE YOUR TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u26a0\ufe0f UPDATE THIS: Your HuggingFace token\n",
    "HF_TOKEN = \"YOUR_HUGGINGFACE_TOKEN_HERE\"\n",
    "\n",
    "# Dataset configuration\n",
    "DATASET_NAME = \"institutional/institutional-books-1.0\"\n",
    "CHUNK_SIZE = 100  # Books per batch\n",
    "MAX_CHUNKS = 10   # Set to None for all books\n",
    "\n",
    "# Database configuration\n",
    "DB_CONFIG = {\n",
    "    'host': 'localhost',\n",
    "    'port': 5432,\n",
    "    'database': 'book_library',\n",
    "    'user': 'bookuser',\n",
    "    'password': 'bookpass123'\n",
    "}\n",
    "\n",
    "# Progress tracking\n",
    "PROGRESS_FILE = 'load_progress.json'\n",
    "\n",
    "print(f\"\ud83d\udcda Dataset: {DATASET_NAME}\")\n",
    "print(f\"\ud83d\udce6 Chunk size: {CHUNK_SIZE}\")\n",
    "print(f\"\ud83d\udd22 Max chunks: {MAX_CHUNKS if MAX_CHUNKS else 'All'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_isbn(identifiers_src: Dict) -> str:\n",
    "    \"\"\"Extract ISBN from identifiers_src field.\"\"\"\n",
    "    if not identifiers_src or 'isbn' not in identifiers_src:\n",
    "        return None\n",
    "    isbns = identifiers_src['isbn']\n",
    "    if isinstance(isbns, list) and len(isbns) > 0:\n",
    "        isbn = str(isbns[0]).strip().replace('-', '').replace(' ', '')\n",
    "        if len(isbn) in [10, 13]:\n",
    "            return isbn\n",
    "    return None\n",
    "\n",
    "def generate_isbn_from_barcode(barcode: str) -> str:\n",
    "    \"\"\"Generate pseudo-ISBN from barcode.\"\"\"\n",
    "    barcode_hash = abs(hash(barcode)) % (10 ** 10)\n",
    "    return f\"999{barcode_hash:010d}\"\n",
    "\n",
    "def parse_date(date_str: str) -> Optional[str]:\n",
    "    \"\"\"Parse date1_src to YYYY-MM-DD format.\"\"\"\n",
    "    if not date_str:\n",
    "        return None\n",
    "    year_match = re.search(r'\\b(1[0-9]{3}|20[0-9]{2})\\b', str(date_str))\n",
    "    if year_match:\n",
    "        return f\"{year_match.group(1)}-01-01\"\n",
    "    return None\n",
    "\n",
    "def clean_text(text: str, max_length: int = 2000) -> str:\n",
    "    \"\"\"Clean and truncate text.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    if len(text) > max_length:\n",
    "        text = text[:max_length] + \"...\"\n",
    "    return text\n",
    "\n",
    "def map_language(lang_code: str) -> str:\n",
    "    \"\"\"Map ISO 639-3 to full language name.\"\"\"\n",
    "    lang_map = {\n",
    "        'eng': 'English', 'fra': 'French', 'deu': 'German',\n",
    "        'spa': 'Spanish', 'ita': 'Italian', 'por': 'Portuguese',\n",
    "        'rus': 'Russian', 'jpn': 'Japanese', 'chi': 'Chinese'\n",
    "    }\n",
    "    return lang_map.get(lang_code[:3].lower(), lang_code or 'English')\n",
    "\n",
    "def extract_genres(genre_or_form_src: str, topic_or_subject_gen: str) -> List[str]:\n",
    "    \"\"\"Extract genres from dataset fields.\"\"\"\n",
    "    genres = []\n",
    "    if genre_or_form_src:\n",
    "        parts = re.split(r'[;,|]', genre_or_form_src)\n",
    "        genres.extend([g.strip() for g in parts if g.strip()])\n",
    "    if topic_or_subject_gen and topic_or_subject_gen not in genres:\n",
    "        genres.append(topic_or_subject_gen)\n",
    "    return genres[:3] if genres else ['General']\n",
    "\n",
    "def fetch_cover_image(isbn: str) -> Optional[str]:\n",
    "    \"\"\"Fetch cover image from Open Library.\"\"\"\n",
    "    if not isbn or isbn.startswith('999'):\n",
    "        return None\n",
    "    try:\n",
    "        url = f\"https://covers.openlibrary.org/b/isbn/{isbn}-L.jpg\"\n",
    "        response = requests.head(url, timeout=2)\n",
    "        if response.status_code == 200:\n",
    "            return url\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "print(\"\u2705 Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Progress Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_progress() -> Dict:\n",
    "    if os.path.exists(PROGRESS_FILE):\n",
    "        with open(PROGRESS_FILE, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {'last_index': -1, 'total_loaded': 0, 'timestamp': None}\n",
    "\n",
    "def save_progress(index: int, total: int):\n",
    "    with open(PROGRESS_FILE, 'w') as f:\n",
    "        json.dump({\n",
    "            'last_index': index,\n",
    "            'total_loaded': total,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }, f, indent=2)\n",
    "\n",
    "print(\"\u2705 Progress tracking defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Login to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login(token=HF_TOKEN)\n",
    "print(\"\u2705 Logged in to HuggingFace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Load Dataset with Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress = load_progress()\n",
    "start_index = progress['last_index'] + 1\n",
    "\n",
    "if start_index > 0:\n",
    "    print(f\"\ud83d\udccd Resuming from index {start_index}\")\n",
    "\n",
    "print(f\"\ud83d\udce5 Loading dataset: {DATASET_NAME}\")\n",
    "dataset_stream = load_dataset(DATASET_NAME, split=\"train\", streaming=True)\n",
    "print(\"\u2705 Dataset loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Connect to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(**DB_CONFIG)\n",
    "cursor = conn.cursor()\n",
    "print(\"\u2705 Database connected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Process Data with Pandas\n",
    "\n",
    "This cell:\n",
    "1. Collects books into chunks\n",
    "2. Creates pandas DataFrame with dataset fields\n",
    "3. Transforms and renames to database schema\n",
    "4. Inserts into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n\ud83d\ude80 Starting data loading\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "chunk_buffer = []\n",
    "current_index = 0\n",
    "chunk_number = 0\n",
    "total_inserted = progress['total_loaded']\n",
    "total_skipped = 0\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    for book_data in dataset_stream:\n",
    "        if current_index < start_index:\n",
    "            current_index += 1\n",
    "            continue\n",
    "        \n",
    "        chunk_buffer.append(book_data)\n",
    "        current_index += 1\n",
    "        \n",
    "        if len(chunk_buffer) >= CHUNK_SIZE:\n",
    "            chunk_number += 1\n",
    "            print(f\"\\n\ud83d\udce6 Chunk {chunk_number} (books {current_index - CHUNK_SIZE + 1}-{current_index})\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            # Create DataFrame from chunk\n",
    "            df = pd.DataFrame(chunk_buffer)\n",
    "            print(f\"   \ud83d\udcca DataFrame created: {len(df)} rows\")\n",
    "            \n",
    "            # Transform data\n",
    "            df['isbn'] = df.apply(lambda row: extract_isbn(row.get('identifiers_src')) or generate_isbn_from_barcode(row.get('barcode_src', '')), axis=1)\n",
    "            df['titre'] = df['title_src'].fillna('Unknown Title')\n",
    "            df['author_names'] = df['author_src'].apply(lambda x: [x] if x else ['Unknown Author'])\n",
    "            df['date_publication'] = df['date1_src'].apply(parse_date)\n",
    "            df['description'] = df['general_note_src'].apply(lambda x: clean_text(x) if x else 'No description')\n",
    "            df['nombre_pages'] = df['page_count_src'].fillna(0).astype(int)\n",
    "            df['total_pages'] = df['nombre_pages']\n",
    "            df['langue'] = df['language_gen'].fillna(df['language_src']).apply(map_language)\n",
    "            df['word_count'] = (df['token_count_o200k_base_gen'].fillna(0) * 0.75).astype(int)\n",
    "            df['genre_names'] = df.apply(lambda row: extract_genres(row.get('genre_or_form_src'), row.get('topic_or_subject_gen')), axis=1)\n",
    "            df['image_url'] = df['isbn'].apply(fetch_cover_image)\n",
    "            \n",
    "            # Select and rename columns for database\n",
    "            db_df = df[[\n",
    "                'isbn', 'titre', 'date_publication', 'description', 'image_url',\n",
    "                'nombre_pages', 'total_pages', 'langue', 'word_count',\n",
    "                'author_names', 'genre_names'\n",
    "            ]].copy()\n",
    "            \n",
    "            # Remove duplicates by ISBN\n",
    "            db_df = db_df.drop_duplicates(subset=['isbn'], keep='first')\n",
    "            \n",
    "            # Check existing ISBNs\n",
    "            existing_isbns = set()\n",
    "            cursor.execute(\"SELECT isbn FROM books WHERE isbn = ANY(%s)\", (db_df['isbn'].tolist(),))\n",
    "            existing_isbns = {row[0] for row in cursor.fetchall()}\n",
    "            \n",
    "            # Filter out existing\n",
    "            db_df = db_df[~db_df['isbn'].isin(existing_isbns)]\n",
    "            \n",
    "            if len(db_df) == 0:\n",
    "                print(f\"   \u23ed\ufe0f  All {len(chunk_buffer)} books already exist\")\n",
    "                total_skipped += len(chunk_buffer)\n",
    "            else:\n",
    "                # Insert books\n",
    "                insert_query = \"\"\"\n",
    "                    INSERT INTO books (\n",
    "                        isbn, titre, date_publication, description, image_url,\n",
    "                        nombre_pages, total_pages, langue, word_count,\n",
    "                        author_names, genre_names,\n",
    "                        note_moyenne, nombre_reviews, average_rating, review_count,\n",
    "                        created_at\n",
    "                    ) VALUES %s\n",
    "                    ON CONFLICT (isbn) DO NOTHING\n",
    "                \"\"\"\n",
    "                \n",
    "                values = [\n",
    "                    (\n",
    "                        row['isbn'], row['titre'], row['date_publication'],\n",
    "                        row['description'], row['image_url'],\n",
    "                        row['nombre_pages'], row['total_pages'], row['langue'],\n",
    "                        row['word_count'], row['author_names'], row['genre_names'],\n",
    "                        0.0, 0, 0.0, 0, datetime.now()\n",
    "                    )\n",
    "                    for _, row in db_df.iterrows()\n",
    "                ]\n",
    "                \n",
    "                execute_values(cursor, insert_query, values)\n",
    "                conn.commit()\n",
    "                \n",
    "                inserted = len(db_df)\n",
    "                skipped = len(chunk_buffer) - inserted\n",
    "                total_inserted += inserted\n",
    "                total_skipped += skipped\n",
    "                \n",
    "                print(f\"   \u2705 Inserted: {inserted}\")\n",
    "                print(f\"   \u23ed\ufe0f  Skipped: {skipped}\")\n",
    "            \n",
    "            # Save progress\n",
    "            save_progress(current_index - 1, total_inserted)\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            rate = total_inserted / elapsed if elapsed > 0 else 0\n",
    "            print(f\"   \ud83d\udcca Total: {total_inserted} loaded, {total_skipped} skipped\")\n",
    "            print(f\"   \u23f1\ufe0f  Rate: {rate:.1f} books/sec\")\n",
    "            \n",
    "            chunk_buffer = []\n",
    "            \n",
    "            if MAX_CHUNKS and chunk_number >= MAX_CHUNKS:\n",
    "                print(f\"\\n\ud83c\udfc1 Reached max chunks ({MAX_CHUNKS})\")\n",
    "                break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\u26a0\ufe0f  Interrupted\")\n",
    "    conn.commit()\n",
    "except Exception as e:\n",
    "    print(f\"\\n\u274c Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    conn.rollback()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\u2705 LOADING COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\ud83d\udcca Total inserted: {total_inserted}\")\n",
    "print(f\"\u23ed\ufe0f  Total skipped: {total_skipped}\")\n",
    "print(f\"\ud83d\udce6 Chunks: {chunk_number}\")\n",
    "print(f\"\u23f1\ufe0f  Time: {elapsed:.1f}s\")\n",
    "print(f\"\ud83d\udcc8 Rate: {total_inserted / elapsed:.1f} books/sec\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Close Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.close()\n",
    "conn.close()\n",
    "print(\"\ud83d\udd0c Database closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: View Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(**DB_CONFIG)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "print(\"\\n\ud83d\udcca Database Summary:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "cursor.execute(\"SELECT COUNT(*) FROM books\")\n",
    "print(f\"\ud83d\udcda Total Books: {cursor.fetchone()[0]}\")\n",
    "\n",
    "cursor.execute(\"SELECT COUNT(*) FROM books WHERE image_url IS NOT NULL\")\n",
    "print(f\"\ud83d\uddbc\ufe0f  Books with covers: {cursor.fetchone()[0]}\")\n",
    "\n",
    "cursor.execute(\"SELECT langue, COUNT(*) FROM books GROUP BY langue ORDER BY COUNT(*) DESC LIMIT 5\")\n",
    "print(\"\\n\ud83c\udf0d Top 5 Languages:\")\n",
    "for lang, count in cursor.fetchall():\n",
    "    print(f\"   {lang}: {count} books\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n",
    "print(\"\\n\u2705 Summary complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}