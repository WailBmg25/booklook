{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Institutional Books Dataset Loader\n",
    "\n",
    "Load books from HuggingFace **institutional/institutional-books-1.0** dataset into BookLook database.\n",
    "\n",
    "## Features\n",
    "- ‚úÖ Streaming mode (memory efficient)\n",
    "- ‚úÖ Chunked loading (100 books per batch)\n",
    "- ‚úÖ Resume capability\n",
    "- ‚úÖ ISBN lookup (dataset ‚Üí Google Books ‚Üí Open Library ‚Üí generated)\n",
    "- ‚úÖ Cover image fetching\n",
    "- ‚úÖ Progress tracking\n",
    "\n",
    "## Dataset Fields\n",
    "- **barcode_src**: Unique identifier\n",
    "- **title_src**: Book title\n",
    "- **author_src**: Author name(s)\n",
    "- **date1_src/date2_src**: Publication dates\n",
    "- **page_count_src**: Number of pages\n",
    "- **language_src/gen**: Language\n",
    "- **topic_or_subject_src/gen**: Subject/topic\n",
    "- **genre_or_form_src**: Genre\n",
    "- **identifiers_src**: ISBN, LCCN, OCLC\n",
    "- **text_by_page_src/gen**: Full text content\n",
    "- **token_count_o200k_base_gen**: Token count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets huggingface-hub psycopg2-binary requests python-dotenv isbnlib -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from typing import List, Dict, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "import psycopg2\n",
    "import requests\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

,{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv('.env.production')\n",
    "\n",
    "# HuggingFace Authentication\n",
    "HF_TOKEN = os.getenv('HUGGINGFACE_TOKEN', '')  # Set in .env.production\n",
    "DATASET_NAME = \"institutional/institutional-books-1.0\"\n",
    "\n",
    "# Database configuration\n",
    "DB_CONFIG = {\n",
    "    'host': os.getenv('POSTGRES_HOST', 'localhost'),\n",
    "    'port': int(os.getenv('POSTGRES_PORT', 5432)),\n",
    "    'database': os.getenv('POSTGRES_DB', 'book_library'),\n",
    "    'user': os.getenv('POSTGRES_USER', 'bookuser'),\n",
    "    'password': os.getenv('POSTGRES_PASSWORD', 'bookpass123')\n",
    "}\n",
    "\n",
    "# Processing configuration\n",
    "CHUNK_SIZE = 100  # Load 100 books at a time\n",
    "MAX_CHUNKS = 10   # Set to None for all books (10 = 1000 books for testing)\n",
    "API_DELAY = 0.5   # Seconds between API calls\n",
    "PROGRESS_FILE = \"load_progress.json\"\n",
    "\n",
    "# Optional: Google Books API Key\n",
    "GOOGLE_BOOKS_API_KEY = os.getenv('GOOGLE_BOOKS_API_KEY', '')\n",
    "\n",
    "print(f\"üìö Dataset: {DATASET_NAME}\")\n",
    "print(f\"üì¶ Chunk size: {CHUNK_SIZE} books per batch\")\n",
    "print(f\"üî¢ Max chunks: {MAX_CHUNKS if MAX_CHUNKS else 'All'}\")\n",
    "print(f\"üîå Database: {DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\")"
   ]
  }

,{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Login to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace\n",
    "login(token=HF_TOKEN)\n",
    "print(\"‚úÖ Successfully logged in to HuggingFace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Helper Functions for ISBN and Cover Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_isbn_from_identifiers(identifiers: Dict) -> Optional[str]:\n",
    "    \"\"\"Extract ISBN from identifiers dictionary.\"\"\"\n",
    "    if not identifiers:\n",
    "        return None\n",
    "    \n",
    "    if 'isbn' in identifiers and identifiers['isbn']:\n",
    "        isbns = identifiers['isbn']\n",
    "        if isinstance(isbns, list) and len(isbns) > 0:\n",
    "            isbn = str(isbns[0]).strip().replace('-', '').replace(' ', '')\n",
    "            if len(isbn) in [10, 13]:\n",
    "                return isbn\n",
    "    return None\n",
    "\n",
    "def search_isbn_google_books(title: str, author: str) -> Optional[str]:\n",
    "    \"\"\"Search for ISBN using Google Books API.\"\"\"\n",
    "    if not title:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        query = f\"{title}\"\n",
    "        if author:\n",
    "            query += f\" {author}\"\n",
    "        \n",
    "        url = \"https://www.googleapis.com/books/v1/volumes\"\n",
    "        params = {'q': query, 'maxResults': 1}\n",
    "        \n",
    "        if GOOGLE_BOOKS_API_KEY:\n",
    "            params['key'] = GOOGLE_BOOKS_API_KEY\n",
    "        \n",
    "        response = requests.get(url, params=params, timeout=5)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if 'items' in data and len(data['items']) > 0:\n",
    "                volume_info = data['items'][0].get('volumeInfo', {})\n",
    "                identifiers = volume_info.get('industryIdentifiers', [])\n",
    "                \n",
    "                for identifier in identifiers:\n",
    "                    if identifier.get('type') == 'ISBN_13':\n",
    "                        return identifier.get('identifier')\n",
    "                \n",
    "                for identifier in identifiers:\n",
    "                    if identifier.get('type') == 'ISBN_10':\n",
    "                        return identifier.get('identifier')\n",
    "        \n",
    "        time.sleep(API_DELAY)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "def search_isbn_open_library(title: str, author: str) -> Optional[str]:\n",
    "    \"\"\"Search for ISBN using Open Library API.\"\"\"\n",
    "    if not title:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        query = title\n",
    "        if author:\n",
    "            query += f\" {author}\"\n",
    "        \n",
    "        url = \"https://openlibrary.org/search.json\"\n",
    "        params = {'q': query, 'limit': 1}\n",
    "        \n",
    "        response = requests.get(url, params=params, timeout=5)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if 'docs' in data and len(data['docs']) > 0:\n",
    "                doc = data['docs'][0]\n",
    "                \n",
    "                if 'isbn' in doc and doc['isbn']:\n",
    "                    for isbn in doc['isbn']:\n",
    "                        isbn_clean = str(isbn).strip().replace('-', '')\n",
    "                        if len(isbn_clean) == 13:\n",
    "                            return isbn_clean\n",
    "                    return str(doc['isbn'][0]).strip().replace('-', '')\n",
    "        \n",
    "        time.sleep(API_DELAY)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "def generate_isbn_from_barcode(barcode: str) -> str:\n",
    "    \"\"\"Generate a pseudo-ISBN from barcode.\"\"\"\n",
    "    barcode_hash = abs(hash(barcode)) % (10 ** 10)\n",
    "    return f\"999{barcode_hash:010d}\"\n",
    "\n",
    "def get_isbn_for_book(book_data: Dict) -> str:\n",
    "    \"\"\"Get or generate ISBN for a book.\"\"\"\n",
    "    # 1. Try to extract from identifiers\n",
    "    isbn = extract_isbn_from_identifiers(book_data.get('identifiers_src'))\n",
    "    if isbn:\n",
    "        return isbn\n",
    "    \n",
    "    # 2. Try Google Books API\n",
    "    title = book_data.get('title_src', '')\n",
    "    author = book_data.get('author_src', '')\n",
    "    \n",
    "    isbn = search_isbn_google_books(title, author)\n",
    "    if isbn:\n",
    "        return isbn\n",
    "    \n",
    "    # 3. Try Open Library API\n",
    "    isbn = search_isbn_open_library(title, author)\n",
    "    if isbn:\n",
    "        return isbn\n",
    "    \n",
    "    # 4. Generate from barcode\n",
    "    barcode = book_data.get('barcode_src', '')\n",
    "    return generate_isbn_from_barcode(barcode)\n",
    "\n",
    "def fetch_cover_image(isbn: str, title: str = \"\") -> Optional[str]:\n",
    "    \"\"\"Fetch cover image URL from multiple sources.\"\"\"\n",
    "    # Try Open Library Covers API\n",
    "    try:\n",
    "        url = f\"https://covers.openlibrary.org/b/isbn/{isbn}-L.jpg\"\n",
    "        response = requests.head(url, timeout=3)\n",
    "        if response.status_code == 200:\n",
    "            return url\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Try Google Books API\n",
    "    if GOOGLE_BOOKS_API_KEY:\n",
    "        try:\n",
    "            url = \"https://www.googleapis.com/books/v1/volumes\"\n",
    "            params = {'q': f'isbn:{isbn}', 'key': GOOGLE_BOOKS_API_KEY}\n",
    "            response = requests.get(url, params=params, timeout=5)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if 'items' in data and len(data['items']) > 0:\n",
    "                    image_links = data['items'][0].get('volumeInfo', {}).get('imageLinks', {})\n",
    "                    if 'thumbnail' in image_links:\n",
    "                        return image_links['thumbnail']\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"‚úÖ Helper functions defined\")"
   ]
  }

,{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Data Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_publication_date(date_str: str) -> Optional[str]:\n",
    "    \"\"\"Parse publication date to YYYY-MM-DD format.\"\"\"\n",
    "    if not date_str:\n",
    "        return None\n",
    "    year_match = re.search(r'\\b(1[0-9]{3}|20[0-9]{2})\\b', str(date_str))\n",
    "    if year_match:\n",
    "        year = year_match.group(1)\n",
    "        return f\"{year}-01-01\"\n",
    "    return None\n",
    "\n",
    "def clean_text(text: str, max_length: int = 5000) -> str:\n",
    "    \"\"\"Clean and truncate text.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    if len(text) > max_length:\n",
    "        text = text[:max_length] + \"...\"\n",
    "    return text\n",
    "\n",
    "def extract_language(book_data: Dict) -> str:\n",
    "    \"\"\"Extract language from book data.\"\"\"\n",
    "    lang = book_data.get('language_gen', '') or book_data.get('language_src', '')\n",
    "    lang_map = {\n",
    "        'eng': 'English', 'fra': 'French', 'deu': 'German',\n",
    "        'spa': 'Spanish', 'ita': 'Italian', 'por': 'Portuguese',\n",
    "        'rus': 'Russian', 'jpn': 'Japanese', 'chi': 'Chinese', 'ara': 'Arabic'\n",
    "    }\n",
    "    return lang_map.get(lang[:3].lower(), lang or 'English')\n",
    "\n",
    "def extract_genres(book_data: Dict) -> List[str]:\n",
    "    \"\"\"Extract genres from book data.\"\"\"\n",
    "    genres = []\n",
    "    genre_str = book_data.get('genre_or_form_src', '')\n",
    "    if genre_str:\n",
    "        parts = re.split(r'[;,|]', genre_str)\n",
    "        genres.extend([g.strip() for g in parts if g.strip()])\n",
    "    \n",
    "    topic = book_data.get('topic_or_subject_gen') or book_data.get('topic_or_subject_src', '')\n",
    "    if topic and topic not in genres:\n",
    "        genres.append(topic)\n",
    "    \n",
    "    if not genres:\n",
    "        genres = ['General']\n",
    "    return genres[:3]\n",
    "\n",
    "def extract_description(book_data: Dict) -> str:\n",
    "    \"\"\"Extract or generate description from book data.\"\"\"\n",
    "    desc = book_data.get('general_note_src', '')\n",
    "    if not desc:\n",
    "        parts = []\n",
    "        topic = book_data.get('topic_or_subject_gen') or book_data.get('topic_or_subject_src')\n",
    "        if topic:\n",
    "            parts.append(f\"Subject: {topic}\")\n",
    "        lang = book_data.get('language_gen', '')\n",
    "        if lang:\n",
    "            parts.append(f\"Language: {lang}\")\n",
    "        pages = book_data.get('page_count_src')\n",
    "        if pages:\n",
    "            parts.append(f\"{pages} pages\")\n",
    "        desc = \". \".join(parts) if parts else \"No description available\"\n",
    "    return clean_text(desc, 2000)\n",
    "\n",
    "print(\"‚úÖ Data processing functions defined\")"
   ]
  }

,{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Database Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_author(cursor, author_name: str) -> int:\n",
    "    \"\"\"Get or create author and return ID.\"\"\"\n",
    "    if not author_name or author_name.strip() == '':\n",
    "        author_name = \"Unknown Author\"\n",
    "    \n",
    "    parts = author_name.strip().split()\n",
    "    if len(parts) >= 2:\n",
    "        prenom = ' '.join(parts[:-1])\n",
    "        nom = parts[-1]\n",
    "    else:\n",
    "        prenom = \"\"\n",
    "        nom = author_name.strip()\n",
    "    \n",
    "    cursor.execute(\"SELECT id FROM authors WHERE nom = %s AND prenom = %s\", (nom, prenom))\n",
    "    result = cursor.fetchone()\n",
    "    \n",
    "    if result:\n",
    "        return result[0]\n",
    "    \n",
    "    cursor.execute(\n",
    "        \"INSERT INTO authors (nom, prenom, created_at) VALUES (%s, %s, NOW()) RETURNING id\",\n",
    "        (nom, prenom)\n",
    "    )\n",
    "    return cursor.fetchone()[0]\n",
    "\n",
    "def get_or_create_genre(cursor, genre_name: str) -> int:\n",
    "    \"\"\"Get or create genre and return ID.\"\"\"\n",
    "    if not genre_name or genre_name.strip() == '':\n",
    "        genre_name = \"General\"\n",
    "    \n",
    "    genre_name = genre_name.strip()\n",
    "    cursor.execute(\"SELECT id FROM genres WHERE nom = %s\", (genre_name,))\n",
    "    result = cursor.fetchone()\n",
    "    \n",
    "    if result:\n",
    "        return result[0]\n",
    "    \n",
    "    cursor.execute(\n",
    "        \"INSERT INTO genres (nom, created_at) VALUES (%s, NOW()) RETURNING id\",\n",
    "        (genre_name,)\n",
    "    )\n",
    "    return cursor.fetchone()[0]\n",
    "\n",
    "print(\"‚úÖ Database helper functions defined\")"
   ]
  }

,{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Progress Tracking Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_progress() -> Dict:\n",
    "    \"\"\"Load progress from file.\"\"\"\n",
    "    if os.path.exists(PROGRESS_FILE):\n",
    "        with open(PROGRESS_FILE, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {\n",
    "        'last_processed_index': -1,\n",
    "        'total_loaded': 0,\n",
    "        'last_barcode': None,\n",
    "        'timestamp': None\n",
    "    }\n",
    "\n",
    "def save_progress(index: int, total: int, barcode: str):\n",
    "    \"\"\"Save progress to file.\"\"\"\n",
    "    progress = {\n",
    "        'last_processed_index': index,\n",
    "        'total_loaded': total,\n",
    "        'last_barcode': barcode,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    with open(PROGRESS_FILE, 'w') as f:\n",
    "        json.dump(progress, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Progress tracking functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Load Dataset with Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load progress\n",
    "progress = load_progress()\n",
    "start_index = progress['last_processed_index'] + 1\n",
    "\n",
    "if start_index > 0:\n",
    "    print(f\"üìç Resuming from index {start_index} ({progress['total_loaded']} books loaded)\")\n",
    "    print(f\"   Last processed: {progress.get('last_barcode', 'N/A')}\")\n",
    "    print(f\"   Timestamp: {progress.get('timestamp', 'N/A')}\\n\")\n",
    "\n",
    "# Load dataset with streaming\n",
    "print(f\"üì• Loading dataset: {DATASET_NAME}\")\n",
    "print(\"   Using streaming mode for memory efficiency...\\n\")\n",
    "\n",
    "dataset_stream = load_dataset(DATASET_NAME, split=\"train\", streaming=True)\n",
    "\n",
    "print(\"‚úÖ Dataset loaded successfully\")"
   ]
  }

,{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Connect to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database\n",
    "print(\"üîå Connecting to database...\")\n",
    "conn = psycopg2.connect(**DB_CONFIG)\n",
    "cursor = conn.cursor()\n",
    "print(\"‚úÖ Database connected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Process Books in Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process dataset in chunks\n",
    "print(f\"\\nüöÄ Starting data loading (chunk size: {CHUNK_SIZE})\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "chunk_buffer = []\n",
    "current_index = 0\n",
    "chunk_number = 0\n",
    "total_inserted = progress['total_loaded']\n",
    "total_skipped = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for book_data in dataset_stream:\n",
    "    # Skip already processed books\n",
    "    if current_index < start_index:\n",
    "        current_index += 1\n",
    "        continue\n",
    "    \n",
    "    # Add to buffer\n",
    "    chunk_buffer.append(book_data)\n",
    "    current_index += 1\n",
    "    \n",
    "    # Process chunk when buffer is full\n",
    "    if len(chunk_buffer) >= CHUNK_SIZE:\n",
    "        chunk_number += 1\n",
    "        \n",
    "        print(f\"\\nüì¶ Processing Chunk {chunk_number} (books {current_index - CHUNK_SIZE + 1}-{current_index})\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Process each book in chunk\n",
    "        inserted = 0\n",
    "        skipped = 0\n",
    "        \n",
    "        for book in chunk_buffer:\n",
    "            try:\n",
    "                # Get or generate ISBN\n",
    "                isbn = get_isbn_for_book(book)\n",
    "                \n",
    "                # Check if book already exists\n",
    "                cursor.execute(\"SELECT id FROM books WHERE isbn = %s\", (isbn,))\n",
    "                if cursor.fetchone():\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "                \n",
    "                # Extract book information\n",
    "                title = book.get('title_src', 'Unknown Title')\n",
    "                author_name = book.get('author_src', 'Unknown Author')\n",
    "                pub_date = parse_publication_date(book.get('date1_src', ''))\n",
    "                description = extract_description(book)\n",
    "                page_count = book.get('page_count_src')\n",
    "                language = extract_language(book)\n",
    "                genres = extract_genres(book)\n",
    "                \n",
    "                # Fetch cover image\n",
    "                cover_url = fetch_cover_image(isbn, title)\n",
    "                \n",
    "                # Get word count from token count (approximate)\n",
    "                token_count = book.get('token_count_o200k_base_gen', 0)\n",
    "                word_count = int(token_count * 0.75) if token_count else None\n",
    "                \n",
    "                # Insert book\n",
    "                cursor.execute(\n",
    "                    \"\"\"\n",
    "                    INSERT INTO books (\n",
    "                        titre, isbn, date_publication, description, image_url,\n",
    "                        nombre_pages, total_pages, langue, note_moyenne, nombre_reviews,\n",
    "                        average_rating, review_count, word_count, created_at\n",
    "                    )\n",
    "                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW())\n",
    "                    RETURNING id\n",
    "                    \"\"\",\n",
    "                    (\n",
    "                        title, isbn, pub_date, description, cover_url,\n",
    "                        page_count, page_count, language, 0.0, 0,\n",
    "                        0.0, 0, word_count\n",
    "                    )\n",
    "                )\n",
    "                book_id = cursor.fetchone()[0]\n",
    "                \n",
    "                # Create or get author\n",
    "                author_id = get_or_create_author(cursor, author_name)\n",
    "                \n",
    "                # Link book to author\n",
    "                cursor.execute(\n",
    "                    \"INSERT INTO book_authors (book_id, author_id) VALUES (%s, %s) ON CONFLICT DO NOTHING\",\n",
    "                    (book_id, author_id)\n",
    "                )\n",
    "                \n",
    "                # Link book to genres\n",
    "                for genre_name in genres:\n",
    "                    genre_id = get_or_create_genre(cursor, genre_name)\n",
    "                    cursor.execute(\n",
    "                        \"INSERT INTO book_genres (book_id, genre_id) VALUES (%s, %s) ON CONFLICT DO NOTHING\",\n",
    "                        (book_id, genre_id)\n",
    "                    )\n",
    "                \n",
    "                # Update denormalized fields\n",
    "                cursor.execute(\n",
    "                    \"UPDATE books SET author_names = ARRAY[%s], genre_names = %s WHERE id = %s\",\n",
    "                    (author_name, genres, book_id)\n",
    "                )\n",
    "                \n",
    "                inserted += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    ‚ùå Error: {e}\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "        \n",
    "        # Commit transaction\n",
    "        conn.commit()\n",
    "        \n",
    "        total_inserted += inserted\n",
    "        total_skipped += skipped\n",
    "        \n",
    "        # Save progress\n",
    "        last_barcode = chunk_buffer[-1].get('barcode_src', 'unknown')\n",
    "        save_progress(current_index - 1, total_inserted, last_barcode)\n",
    "        \n",
    "        # Print stats\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = total_inserted / elapsed if elapsed > 0 else 0\n",
    "        \n",
    "        print(f\"   ‚úÖ Inserted: {inserted}\")\n",
    "        print(f\"   ‚è≠Ô∏è  Skipped: {skipped}\")\n",
    "        print(f\"   üìä Total: {total_inserted} books loaded, {total_skipped} skipped\")\n",
    "        print(f\"   ‚è±Ô∏è  Rate: {rate:.1f} books/second\")\n",
    "        print(f\"   üíæ Progress saved\")\n",
    "        \n",
    "        # Clear buffer\n",
    "        chunk_buffer = []\n",
    "        \n",
    "        # Check if we've reached max chunks\n",
    "        if MAX_CHUNKS and chunk_number >= MAX_CHUNKS:\n",
    "            print(f\"\\nüèÅ Reached maximum chunks limit ({MAX_CHUNKS})\")\n",
    "            break\n",
    "\n",
    "# Final summary\n",
    "elapsed = time.time() - start_time\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ LOADING COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"üìä Total books inserted: {total_inserted}\")\n",
    "print(f\"‚è≠Ô∏è  Total books skipped: {total_skipped}\")\n",
    "print(f\"üì¶ Total chunks processed: {chunk_number}\")\n",
    "print(f\"‚è±Ô∏è  Total time: {elapsed:.1f} seconds\")\n",
    "print(f\"üìà Average rate: {total_inserted / elapsed:.1f} books/second\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Close Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close database connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "print(\"üîå Database connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: View Loaded Data Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconnect to view summary\n",
    "conn = psycopg2.connect(**DB_CONFIG)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "print(\"\\nüìä Database Summary:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "cursor.execute(\"SELECT COUNT(*) FROM books\")\n",
    "print(f\"üìö Total Books: {cursor.fetchone()[0]}\")\n",
    "\n",
    "cursor.execute(\"SELECT COUNT(*) FROM authors\")\n",
    "print(f\"‚úçÔ∏è  Total Authors: {cursor.fetchone()[0]}\")\n",
    "\n",
    "cursor.execute(\"SELECT COUNT(*) FROM genres\")\n",
    "print(f\"üè∑Ô∏è  Total Genres: {cursor.fetchone()[0]}\")\n",
    "\n",
    "cursor.execute(\"SELECT COUNT(*) FROM books WHERE image_url IS NOT NULL\")\n",
    "print(f\"üñºÔ∏è  Books with covers: {cursor.fetchone()[0]}\")\n",
    "\n",
    "cursor.execute(\"SELECT langue, COUNT(*) FROM books GROUP BY langue ORDER BY COUNT(*) DESC LIMIT 5\")\n",
    "print(\"\\nüåç Top 5 Languages:\")\n",
    "for lang, count in cursor.fetchall():\n",
    "    print(f\"   {lang}: {count} books\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n",
    "print(\"\\n‚úÖ Summary complete!\")"
   ]
  }
 ]
}
